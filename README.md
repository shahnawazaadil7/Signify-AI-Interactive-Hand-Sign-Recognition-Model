ğŸ¤Ÿ Real-Time Indian Sign Language (ISL) Translator

Smart India Hackathon 2024 Project
Developed using Dense Neural Networks, MediaPipe, and OpenCV

ğŸ“ Overview

This project presents a real-time hand sign recognition system designed for Indian Sign Language (ISL) translation. Built with a combination of MediaPipe for hand tracking and TensorFlow/Keras for gesture classification, this system can capture and classify live hand gestures using a webcam.

Developed by a team of 6 CSE-DS students from LIET for the Smart India Hackathon 2024, the project involved collecting and training on 57,600 hand gesture images.

â¸»

ğŸ“‚ Folder Structure
```
.
â”œâ”€â”€ data/                   # Folder containing captured images, organized per class
â”œâ”€â”€ models/                 # Folder to store the trained model and preprocessed data
â”‚   â”œâ”€â”€ dnn_model.p         # Trained Dense Neural Network model
â”‚   â”œâ”€â”€ label_encoder.p     # Saved LabelEncoder object
â”‚   â””â”€â”€ preprocessed_data.p # Pickled data after preprocessing
â”œâ”€â”€ capture_data.py         # Script to collect gesture images via webcam
â”œâ”€â”€ preprocess_data.py      # Script to extract hand landmarks and label data
â”œâ”€â”€ train_model.py          # Script to train the DNN model
â”œâ”€â”€ run_realtime_demo.py    # Real-time prediction using webcam and trained model
â””â”€â”€ README.md               # Project documentation

```
â¸»

ğŸš€ Features
	â€¢	Captures hand gesture data with webcam
	â€¢	Detects hand landmarks using MediaPipe
	â€¢	Extracts and normalizes 84 hand landmark coordinates (42 for each hand)
	â€¢	Includes hand presence as an additional feature
	â€¢	Trains a Dense Neural Network (DNN) to classify 36 ISL signs
	â€¢	Real-time prediction and feedback using OpenCV and the trained model
	â€¢	Total dataset: 57,600 images (36 classes Ã— 1600 images per class approx.)

â¸»

ğŸ”§ Setup Instructions

1. Clone the Repository
```
git clone https://github.com/shahnawazaadil7/Signify-AI-Interactive-Hand-Sign-Recognition-Model/
cd Signify-AI-Interactive-Hand-Sign-Recognition-Model
```
2. Install Dependencies
```
pip install opencv-python mediapipe scikit-learn numpy tensorflow
```

â¸»

ğŸ“¸ Data Collection

To capture gesture data for training:
```
python capture_data.py
```
â€¢	You will be prompted class-wise to collect 100 samples per gesture.
â€¢	Press Q to start capturing for each class.

â¸»

ğŸ” Preprocessing

To process and extract landmarks:
```
python preprocess_data.py
```
â€¢	Uses MediaPipe to detect hands and extract 84 hand landmark coordinates.
â€¢	Pads data if only one hand is present and includes a hand presence flag.
â€¢	Saves the feature vectors and labels to models/preprocessed_data.p.

â¸»

ğŸ§  Training the Model

To train the Dense Neural Network:
```
python train_model.py
```
â€¢	Trains on the processed dataset with an 80/20 train-test split.
â€¢	Uses dropout and ReLU activation to reduce overfitting.
â€¢	Saves the trained model as models/dnn_model.p.

â¸»

ğŸ–¥ï¸ Real-Time Prediction

To run the live demo:
```
python run_realtime_demo.py
```
â€¢	Opens your webcam and predicts ISL signs in real time.
â€¢	Displays bounding boxes and prediction confidence on-screen.

â¸»

ğŸ§ª Model Architecture
	â€¢	Input Layer: 85 features (84 landmark points + 1 hand presence flag)
	â€¢	Hidden Layers:
	â€¢	Dense(128) â†’ ReLU
	â€¢	Dense(64) â†’ ReLU â†’ Dropout(0.3)
	â€¢	Dense(32) â†’ ReLU â†’ Dropout(0.3)
	â€¢	Output Layer: Softmax over 36 classes

â¸»

ğŸ† Hackathon Details
	â€¢	Event: Smart India Hackathon 2024
	â€¢	Team Size: 6
	â€¢	Track: Accessibility / Sign Language Translation
	â€¢	Dataset Size: 57,600 gesture images
	â€¢	Tools Used: Python, TensorFlow, OpenCV, MediaPipe, scikit-learn

â¸»

ğŸ“Œ Future Improvements
	â€¢	Add multi-word sentence formation using LSTM or attention models
	â€¢	Implement hand orientation and depth normalization
	â€¢	Deploy the system as a web or mobile app
	â€¢	Incorporate dynamic gestures (like movement-based signs)

â¸»

ğŸ“ƒ License

This project is open-sourced under the MIT License.
